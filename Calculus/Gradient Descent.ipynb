{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFnroMwp5sUxeUzQCmvyjw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Gradient Descent\n","\n","Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving towards the steepest descent, or the direction of the negative gradient.\n","\n","- [ 1 - Optimization Using Gradient Descent in One Variable](#1)\n","- [ 2 - Optimization Using Gradient Descent in Two Variables](#2)"],"metadata":{"id":"wOdLuoeN39NI"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"cwE3rrgQ33ud","executionInfo":{"status":"ok","timestamp":1750147088815,"user_tz":-330,"elapsed":416,"user":{"displayName":"Mounika Alwar","userId":"08184990801496821107"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["<a name='1'></a>\n","## 1 - Optimization Using Gradient Descent in One Variable"],"metadata":{"id":"fBo-Mcii5SSX"}},{"cell_type":"markdown","source":["\n","\n","Consider function $f\\left(x\\right)=e^x - \\log(x)$ (defined for $x>0$) is a function of one variable which has only one minimum point (called global minimum)."],"metadata":{"id":"hWxXQVVq5h7i"}},{"cell_type":"code","source":["def f(x):\n","  return np.exp(x) - np.log(x)\n","\n","def dfdx(x):\n","  return np.exp(x) - (1/x)"],"metadata":{"id":"11c994Ot5RDg","executionInfo":{"status":"ok","timestamp":1750148481105,"user_tz":-330,"elapsed":4,"user":{"displayName":"Mounika Alwar","userId":"08184990801496821107"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def gradient_descent(dfdx,x,learning_rate=0.1,num_iterations=100):\n","  for iteration in range(num_iterations):\n","    x = x - learning_rate * dfdx(x)\n","  return x"],"metadata":{"id":"ornL9xbX6CsG","executionInfo":{"status":"ok","timestamp":1750147540219,"user_tz":-330,"elapsed":642,"user":{"displayName":"Mounika Alwar","userId":"08184990801496821107"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["num_iterations = 25\n","learning_rate = 0.1\n","x_initial = 1.6\n","print(gradient_descent(dfdx,x_initial,learning_rate,num_iterations))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoqlhCSW6SQz","executionInfo":{"status":"ok","timestamp":1750147540219,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mounika Alwar","userId":"08184990801496821107"}},"outputId":"a3cb15db-6e50-4a28-d85c-a5e5d5652746"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["0.5671434156768685\n"]}]},{"cell_type":"markdown","source":["<a name='2'></a>\n","## 2 - Optimization Using Gradient Descent in Two Variables\n","\n","Consider function $f\\left(x\\right)=e^x + y^2 - \\log(x+y)$ (defined for $x>0$ and $y>0$) is a function of two variables which has only one minimum point (called global minimum)."],"metadata":{"id":"X_U1e0O29eVB"}},{"cell_type":"code","source":["def f(x,y):\n","  return np.exp(x) + y**2 - np.log(x+y)\n","\n","def dfdx(x,y):\n","  return np.exp(x) - 1 / (x+y)\n","\n","def dfdy(x,y):\n","  return 2*y - 1 / (x+y)"],"metadata":{"id":"vXZSA6U96qkz","executionInfo":{"status":"ok","timestamp":1750148680703,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mounika Alwar","userId":"08184990801496821107"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def gradient_descent(dfdx,dfdy,x,y,learning_rate=0.1,num_iterations=100):\n","  for iteration in range(num_iterations):\n","    x = x - learning_rate * dfdx(x,y)\n","    y = y - learning_rate * dfdy(x,y)\n","  return x,y"],"metadata":{"id":"jOH2M_7z_YgU","executionInfo":{"status":"ok","timestamp":1750148725045,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mounika Alwar","userId":"08184990801496821107"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["num_iterations = 25\n","learning_rate = 0.1\n","x_initial = 1.6\n","y_initial = 1.2\n","print(gradient_descent(dfdx, dfdy, x_initial, y_initial, learning_rate, num_iterations))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAmdqkh5_jL-","executionInfo":{"status":"ok","timestamp":1750148768126,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mounika Alwar","userId":"08184990801496821107"}},"outputId":"f5033dc0-103d-481e-b5bd-1297e7a33c2b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["(np.float64(0.20696894888689563), np.float64(0.6101990793452488))\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2UGwsdtk_tsj"},"execution_count":null,"outputs":[]}]}